{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0614474",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/boognish/mambaforge/envs/modest-mouser/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from early_stopping import EarlyStopping\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "random.seed(42)  # for consistent results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "634e55f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define training hyperparameters\n",
    "lr = 0.001\n",
    "momentum = 0.9\n",
    "batch_size = 32\n",
    "patience = 2  # stop training when average validation loss stops decreasing for 2 epochs\n",
    "max_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "827c0834",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "## Load in training, validation and testing data\n",
    "transform = transforms.Compose(\n",
    "# convert PIL images in cifar10 dataset to tensors, normalize the images\n",
    "# and augment them using random color modification and flips\n",
    "[transforms.ToTensor(),\n",
    " transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    " transforms.ColorJitter(hue=.05, saturation=.05),\n",
    " transforms.RandomHorizontalFlip()]\n",
    ")\n",
    "\n",
    "# create dataloaders for train, valid and test data\n",
    "# reserve 20% of training data for validation/early stopping\n",
    "train_set = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                         download=True, transform=transform)\n",
    "\n",
    "train_size = int(0.8 * len(train_set))\n",
    "valid_size = len(train_set) - train_size\n",
    "\n",
    "train_set, valid_set = torch.utils.data.random_split(train_set,\n",
    "                                                     [train_size, valid_size])\n",
    "\n",
    "test_set = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                        download=True, transform=transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size,\n",
    "                                           shuffle=True)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_set, batch_size=batch_size,\n",
    "                                           shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size,\n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "018c1687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define LeNet (LeCun et al., 1998) network architecture\n",
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=6,\n",
    "                               kernel_size=5)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=16,\n",
    "                               kernel_size=5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # first convolutional layer, w/ max pooling and ReLU activation\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        \n",
    "        # second convolutional layer, same as first\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        \n",
    "        # flatten output from second conv layer and pass thru\n",
    "        # multi-layer perceptron for classification\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        \n",
    "        # return logits from last layer\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "90881aa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | mean train loss: 2.28528 | mean valid loss: 2.20153\n",
      "\n",
      "Validation loss decreased (inf --> 2.201526).  Saving model ...\n",
      "Epoch 2 | mean train loss: 2.06323 | mean valid loss: 2.00193\n",
      "\n",
      "Validation loss decreased (2.201526 --> 2.001933).  Saving model ...\n",
      "Epoch 3 | mean train loss: 1.93578 | mean valid loss: 1.85871\n",
      "\n",
      "Validation loss decreased (2.001933 --> 1.858709).  Saving model ...\n",
      "Epoch 4 | mean train loss: 1.77820 | mean valid loss: 1.74295\n",
      "\n",
      "Validation loss decreased (1.858709 --> 1.742947).  Saving model ...\n",
      "Epoch 5 | mean train loss: 1.67120 | mean valid loss: 1.65837\n",
      "\n",
      "Validation loss decreased (1.742947 --> 1.658369).  Saving model ...\n",
      "Epoch 6 | mean train loss: 1.61353 | mean valid loss: 1.62099\n",
      "\n",
      "Validation loss decreased (1.658369 --> 1.620988).  Saving model ...\n",
      "Epoch 7 | mean train loss: 1.57179 | mean valid loss: 1.58287\n",
      "\n",
      "Validation loss decreased (1.620988 --> 1.582871).  Saving model ...\n",
      "Epoch 8 | mean train loss: 1.53422 | mean valid loss: 1.56828\n",
      "\n",
      "Validation loss decreased (1.582871 --> 1.568276).  Saving model ...\n",
      "Epoch 9 | mean train loss: 1.50564 | mean valid loss: 1.54357\n",
      "\n",
      "Validation loss decreased (1.568276 --> 1.543573).  Saving model ...\n",
      "Epoch 10 | mean train loss: 1.47799 | mean valid loss: 1.50847\n",
      "\n",
      "Validation loss decreased (1.543573 --> 1.508465).  Saving model ...\n",
      "Epoch 11 | mean train loss: 1.44938 | mean valid loss: 1.47842\n",
      "\n",
      "Validation loss decreased (1.508465 --> 1.478425).  Saving model ...\n",
      "Epoch 12 | mean train loss: 1.42762 | mean valid loss: 1.46940\n",
      "\n",
      "Validation loss decreased (1.478425 --> 1.469398).  Saving model ...\n",
      "Epoch 13 | mean train loss: 1.40053 | mean valid loss: 1.44941\n",
      "\n",
      "Validation loss decreased (1.469398 --> 1.449411).  Saving model ...\n",
      "Epoch 14 | mean train loss: 1.38086 | mean valid loss: 1.48467\n",
      "\n",
      "EarlyStopping counter: 1 out of 2\n",
      "Epoch 15 | mean train loss: 1.36255 | mean valid loss: 1.41115\n",
      "\n",
      "Validation loss decreased (1.449411 --> 1.411149).  Saving model ...\n",
      "Epoch 16 | mean train loss: 1.34297 | mean valid loss: 1.41174\n",
      "\n",
      "EarlyStopping counter: 1 out of 2\n",
      "Epoch 17 | mean train loss: 1.31970 | mean valid loss: 1.39401\n",
      "\n",
      "Validation loss decreased (1.411149 --> 1.394008).  Saving model ...\n",
      "Epoch 18 | mean train loss: 1.30126 | mean valid loss: 1.40835\n",
      "\n",
      "EarlyStopping counter: 1 out of 2\n",
      "Epoch 19 | mean train loss: 1.28813 | mean valid loss: 1.35708\n",
      "\n",
      "Validation loss decreased (1.394008 --> 1.357080).  Saving model ...\n",
      "Epoch 20 | mean train loss: 1.27306 | mean valid loss: 1.35064\n",
      "\n",
      "Validation loss decreased (1.357080 --> 1.350645).  Saving model ...\n",
      "Epoch 21 | mean train loss: 1.25202 | mean valid loss: 1.35627\n",
      "\n",
      "EarlyStopping counter: 1 out of 2\n",
      "Epoch 22 | mean train loss: 1.23754 | mean valid loss: 1.35018\n",
      "\n",
      "Validation loss decreased (1.350645 --> 1.350176).  Saving model ...\n",
      "Epoch 23 | mean train loss: 1.22025 | mean valid loss: 1.34474\n",
      "\n",
      "Validation loss decreased (1.350176 --> 1.344740).  Saving model ...\n",
      "Epoch 24 | mean train loss: 1.21018 | mean valid loss: 1.34587\n",
      "\n",
      "EarlyStopping counter: 1 out of 2\n",
      "Epoch 25 | mean train loss: 1.20202 | mean valid loss: 1.31597\n",
      "\n",
      "Validation loss decreased (1.344740 --> 1.315967).  Saving model ...\n",
      "Epoch 26 | mean train loss: 1.18445 | mean valid loss: 1.30708\n",
      "\n",
      "Validation loss decreased (1.315967 --> 1.307078).  Saving model ...\n",
      "Epoch 27 | mean train loss: 1.17436 | mean valid loss: 1.33363\n",
      "\n",
      "EarlyStopping counter: 1 out of 2\n",
      "Epoch 28 | mean train loss: 1.15814 | mean valid loss: 1.31692\n",
      "\n",
      "EarlyStopping counter: 2 out of 2\n",
      "Early stopping at epoch 28 / 100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Define training loop\n",
    "net = LeNet()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=lr, momentum=momentum)\n",
    "early_stopping = EarlyStopping(patience=patience, verbose=True)\n",
    "\n",
    "# keep track of validation set losses from each epoch\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "avg_train_losses = []\n",
    "avg_valid_losses = []\n",
    "\n",
    "for epoch in range(1, max_epochs + 1):\n",
    "    for i, data in enumerate(train_loader):\n",
    "        inputs, labels = data\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # get model predictions and calculate loss\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # perform backprop using loss and model weights\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # record training losses for this minibatch\n",
    "        train_losses.append(loss.item())\n",
    "        \n",
    "    # after each training epoch, get average validation set loss and\n",
    "    # do early stopping if necessary\n",
    "    #\n",
    "    # i.e: model has stopped improving on the validation set after\n",
    "    # set patience\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in valid_loader:\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            valid_losses.append(loss.item())\n",
    "    \n",
    "    # calculate average training and average losses for this epoch\n",
    "    train_loss = np.average(train_losses)\n",
    "    valid_loss = np.average(valid_losses)\n",
    "    avg_train_losses.append(train_loss)\n",
    "    avg_valid_losses.append(valid_loss)\n",
    "    \n",
    "    print(f\"Epoch {str(epoch)} | mean train loss: {train_loss:.5f} | mean valid loss: {valid_loss:.5f}\\n\")\n",
    "    \n",
    "    # reset list of train/valid losses for next epoch\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "    \n",
    "    # use EarlyStopping object to see if validation loss has stopped\n",
    "    # improving/decreasing for the set patience\n",
    "    early_stopping(valid_loss, net)\n",
    "    \n",
    "    if early_stopping.early_stop:\n",
    "        print(f\"Early stopping at epoch {str(epoch)} / {str(max_epochs)}\")\n",
    "        break\n",
    "\n",
    "# load checkpoint with best model, created by EarlyStopping\n",
    "net.load_state_dict(torch.load('checkpoint.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8f5b147b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network testing accuracy: 55 %\n"
     ]
    }
   ],
   "source": [
    "## Test trained model\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        images, labels = data\n",
    "        outputs = net(images)\n",
    "        \n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Network testing accuracy: {100 * correct // total} %')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:modest-mouser]",
   "language": "python",
   "name": "conda-env-modest-mouser-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
